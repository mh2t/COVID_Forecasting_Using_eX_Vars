{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "import stellargraph as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"https://raw.githubusercontent.com/scalation/data/master/COVID/CLEANED_35_Updated.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"deathIncrease\", \"hospitalizedIncrease\"]\n",
    "x = csv_data[params].iloc[44:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adj = None\n",
    "for i in params:\n",
    "    vals = []\n",
    "    for j in params:\n",
    "        if i==j:\n",
    "            vals.append(1)\n",
    "        else:\n",
    "#             data = x[[i, j]]\n",
    "#             gc_res = grangercausalitytests(data, [10])\n",
    "#             vals.append(gc_res[10][0][\"ssr_ftest\"][0])\n",
    "            vals.append(stats.pearsonr(x[i], x[j])[0])\n",
    "\n",
    "    df = pd.DataFrame([vals], index=[i], columns = params)\n",
    "    if isinstance(adj, pd.DataFrame):\n",
    "        adj = adj.append(df)\n",
    "    else:\n",
    "        adj = df\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = x.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes, time_len = covid_data.shape\n",
    "print(\"No. of states:\", num_nodes, \"\\nNo of timesteps:\", time_len)\n",
    "\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_portion):\n",
    "    time_len = data.shape[1]\n",
    "    train_size = int(time_len * (train_portion - 0.1))\n",
    "    val_size = train_size + int(time_len * 0.1) + 1\n",
    "    train_data = np.array(data.iloc[:, :train_size])\n",
    "    val_data = np.array(data.iloc[:, train_size:val_size])\n",
    "    test_data = np.array(data.iloc[:, val_size:])\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate = 0.6277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = train_test_split(covid_data, train_rate)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Val data: \", val_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train_data, val_data, test_data):\n",
    "    max_deaths = train_data.max()\n",
    "    min_deaths = train_data.min()\n",
    "    train_scaled = (train_data - min_deaths) / (max_deaths - min_deaths)\n",
    "    val_scaled = (val_data - min_deaths) / (max_deaths - min_deaths)\n",
    "    test_scaled = (test_data - min_deaths) / (max_deaths - min_deaths)\n",
    "    return train_scaled, val_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, val_scaled, test_scaled = scale_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "pre_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def sequence_data_preparation(seq_len, pre_len, train_data, val_data, test_data):\n",
    "    trainX, trainY, valX, valY, testX, testY = [], [], [], [], [], []\n",
    "\n",
    "    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        a = train_data[:, i : i + seq_len + pre_len]\n",
    "        trainX.append(a[:, :seq_len])\n",
    "        trainY.append(a[:, -1])\n",
    "\n",
    "    for i in range(val_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        b = val_data[:, i : i + seq_len + pre_len]\n",
    "        valX.append(b[:, :seq_len])\n",
    "        valY.append(b[:, -1])\n",
    "\n",
    "    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        b = test_data[:, i : i + seq_len + pre_len]\n",
    "        testX.append(b[:, :seq_len])\n",
    "        testY.append(b[:, -1])\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    valX = np.array(valX)\n",
    "    valY = np.array(valY)\n",
    "    testX = np.array(testX)\n",
    "    testY = np.array(testY)\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.layer import GCN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_lstm = GCN_LSTM(\n",
    "        seq_len=seq_len,\n",
    "        adj=adj,\n",
    "        gc_layer_sizes=[32, 16],\n",
    "        gc_activations=[\"relu\", \"relu\"],\n",
    "        lstm_layer_sizes=[150],\n",
    "        lstm_activations=[\"tanh\"],\n",
    "    )\n",
    "x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    seq_len = 10\n",
    "    pre_len = i\n",
    "    trainX, trainY, valX, valY, testX, testY = sequence_data_preparation(seq_len, pre_len, train_scaled, val_scaled, test_scaled)\n",
    "    gcn_lstm = GCN_LSTM(\n",
    "        seq_len=seq_len,\n",
    "        adj=adj,\n",
    "        gc_layer_sizes=[32, 16],\n",
    "        gc_activations=[\"relu\", \"relu\"],\n",
    "        lstm_layer_sizes=[50],\n",
    "        lstm_activations=[\"tanh\"],\n",
    "    )\n",
    "    x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=['mse'])\n",
    "    history = model.fit(\n",
    "        trainX,\n",
    "        trainY,\n",
    "        epochs=150,\n",
    "        batch_size=10,\n",
    "        verbose=0,\n",
    "        validation_data=(valX, valY),\n",
    "    )\n",
    "    ythat = model.predict(trainX)\n",
    "    yhat = model.predict(testX)\n",
    "    max_deaths = train_data.max()\n",
    "    min_deaths = train_data.min()\n",
    "\n",
    "    train_rescref = np.array((trainY * (max_deaths - min_deaths)) + min_deaths)\n",
    "    test_rescref = np.array((testY * (max_deaths - min_deaths)) + min_deaths)\n",
    "\n",
    "    train_rescpred = np.array((ythat * (max_deaths - min_deaths)) + min_deaths)\n",
    "    test_rescpred  = np.array((yhat * (max_deaths - min_deaths)) + min_deaths)\n",
    "    pred = test_rescpred[:,0]\n",
    "    true = test_rescref[:,0]\n",
    "    print(s_mean_absolute_percentage_error(true,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
